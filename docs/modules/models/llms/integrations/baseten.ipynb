{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseten\n",
    "\n",
    "[Baseten](https://baseten.co) provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.\n",
    "\n",
    "This example demonstrates using Langchain with models deployed on Baseten."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "To run this notebook, you'll need a [Baseten account](https://baseten.co) and an [API key](https://docs.baseten.co/settings/api-keys).\n",
    "\n",
    "You'll also need to install the Baseten Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install baseten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import baseten\n",
    "\n",
    "baseten.login(\"YOUR_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single model call\n",
    "\n",
    "First, you'll need to deploy a model to Baseten.\n",
    "\n",
    "You can deploy foundation models like LLaMA and FLAN-T5 with one click from the [Baseten model library](https://app.baseten.co/explore/) or if you have your own model, [deploy it with this tutorial](https://docs.baseten.co/deploying-models/deploy).\n",
    "\n",
    "In this example, we'll work with LLaMA. [Deploy LLaMA here](https://app.baseten.co/explore/llama) and follow along with the deployed [model's ID](https://docs.baseten.co/managing-models/manage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Baseten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "llama = Baseten(model=\"MODEL_ID\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the model\n",
    "\n",
    "llama(\"Answer this question: What animals grow wool for making sweaters?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chained model calls\n",
    "\n",
    "We can chain together multiple calls to one or multiple models, which is the whole point of Langchain!\n",
    "\n",
    "This example uses LLaMA to plan meals for a day, write a recipie for each meal, then use those recipies to create a grocery list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the first link in the chain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"food_type\"],\n",
    "    template=\"Plan a {food_type} Dinner.\",\n",
    ")\n",
    "\n",
    "link_one = LLMChain(llm=llama, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the second link in the chain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"meal_plan\"],\n",
    "    template=\"Write a recipe for {meal_plan}.\",\n",
    ")\n",
    "\n",
    "link_two = LLMChain(llm=llama, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the third link in the chain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"recipie\"],\n",
    "    template=\"Write a grocery shopping list for {recipie}.\",\n",
    ")\n",
    "\n",
    "link_three = LLMChain(llm=llama, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full chain!\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[link_one, link_two, link_three], verbose=True)\n",
    "shopping_list = chain.run(\"Japanese\")\n",
    "print(shopping_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
